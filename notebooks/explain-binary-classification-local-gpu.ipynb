{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain binary classification model predictions on GPU\n",
    "_**This notebook showcases how to use the interpret-community repo to help interpret and visualize predictions from a binary classification model on GPU.**_\n",
    "\n",
    "Adapted from `explain-binary-classification-local.ipynb` notebook in the repository\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Project](#Project)\n",
    "1. [Run model explainer locally at training time](#Explain)\n",
    "    1. Train a binary classification model\n",
    "    1. Explain the model\n",
    "        1. Generate global explanations\n",
    "        1. Generate local explanations\n",
    "1. [Visualize results](#Visualize)\n",
    "1. [Next steps](#Next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Introduction'></a>\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook illustrates how to locally use interpret-community to help interpret binary classification model predictions at training time.  It demonstrates the API calls needed to obtain the global and local interpretations along with an interactive visualization dashboard for discovering patterns in data and explanations.\n",
    "\n",
    "Two options using the TabularExplainer on CPU and GPU ( with the `use_gpu` flag) are demonstrated: \n",
    "- KernelExplainer - uses [SHAP KernelExplainer](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html#shap-kernelexplainer) on CPU\n",
    "- GPUKernelExplainer - uses [cuML KernelExplainer](https://docs.rapids.ai/api/cuml/stable/api.html#cuml.explainer.KernelExplainer) for GPU Acceleration\n",
    "\n",
    "To run the GPUKernelExplainer:\n",
    "- Ensure local machine has GPU and CUDA & NVIDIA Drivers installed. For minimum version requirements visit [RAPIDS getting started](https://rapids.ai/start.html)\n",
    "- Install [RAPIDS libraries](https://rapids.ai/start.html#get-rapids)\n",
    "\n",
    "\n",
    "<a id='Project'></a>       \n",
    "## 2. Project\n",
    "\n",
    "The goal of this project is to classify breast cancer diagnosis with scikit-learn and cuML then locally running the model explainer:\n",
    "\n",
    "1. Train a SVM classification model using Scikit-learn\n",
    "2. Run 'explain_model' globally and locally with full dataset in local mode, which doesn't contact any Azure services.\n",
    "3. Visualize the global and local explanations with the visualization dashboard.\n",
    "\n",
    "<a id='Setup'></a>\n",
    "## 3. Setup\n",
    "\n",
    "If you are using Jupyter notebooks, the extensions should be installed automatically with the package.\n",
    "If you are using Jupyter Labs run the following command:\n",
    "```\n",
    "(myenv) $ jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Explain'></a>\n",
    "## 4. Run model explainer locally at training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import svm\n",
    "\n",
    "# Explainers:\n",
    "# 1. SHAP Kernel Explainer\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "\n",
    "# cuML is a Machine Learning library within RAPDIS similar to scikit-learn\n",
    "import cuml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the breast cancer diagnosis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_data = load_breast_cancer()\n",
    "classes = breast_cancer_data.target_names.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(breast_cancer_data.data, breast_cancer_data.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a SVM classification model, which you want to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a cuML model\n",
    "cu_clf = cuml.svm.SVC(gamma=0.001, C=100., probability=True)\n",
    "model = cu_clf.fit(x_train, y_train)\n",
    "\n",
    "# Train sklearn model\n",
    "# clf = svm.SVC(gamma=0.001, C=100., probability=True)\n",
    "# sk_model = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain predictions on your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using GPU SHAP TabularExplainer with model\n",
    "# To use this, the machine should have GPUs present and RAPIDS libraries installed. \n",
    "# Visit https://rapids.ai for more information. This option uses cuML's SHAP implementation on GPU.\n",
    "# cuML is a GPU-accelerated Machine Learning library within RAPDIS and mirrors scikit-learn's API\n",
    "# cuML model with GPU SHAP proved to be the most optimal combination for speed. We noticed it yielded \n",
    "# a 3.5x speed-up over  sklearn model with CPU SHAP on RTX 8000. The speed-ups with larger dataset \n",
    "# will be more significant.\n",
    "explainer = TabularExplainer(model, \n",
    "                             x_train, \n",
    "                             features=breast_cancer_data.feature_names, \n",
    "                             classes=classes,\n",
    "                             use_gpu=True)\n",
    "\n",
    "# 2. Using GPU SHAP TabularExplainer with sklearn model\n",
    "# We can use a model from scikit-learn model for training as well. Train the sklearn model \n",
    "# by uncommenting appropriate lines in the previous cell to run GPU SHAP + sklearn.\n",
    "# explainer = TabularExplainer(sk_model, \n",
    "#                              x_train, \n",
    "#                              features=breast_cancer_data.feature_names, \n",
    "#                              classes=classes,\n",
    "#                              use_gpu=True)\n",
    "\n",
    "# 3. Using CPU SHAP TabularExplainer with sklearn model\n",
    "# explainer = TabularExplainer(sk_model, \n",
    "#                              x_train, \n",
    "#                              features=breast_cancer_data.feature_names, \n",
    "#                              classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate global explanations\n",
    "Explain overall model predictions (global explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing in test dataset for evaluation examples - note it must be a representative sample of the original data\n",
    "# x_train can be passed as well, but with more examples explanations will take longer although they may be more accurate\n",
    "global_explanation = explainer.explain_global(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted SHAP values\n",
    "print('ranked global importance values: {}'.format(global_explanation.get_ranked_global_values()))\n",
    "# Corresponding feature names\n",
    "print('ranked global importance names: {}'.format(global_explanation.get_ranked_global_names()))\n",
    "# Feature ranks (based on original order of features)\n",
    "print('global importance rank: {}'.format(global_explanation.global_importance_rank))\n",
    "\n",
    "# Per class feature names\n",
    "print('ranked per class feature names: {}'.format(global_explanation.get_ranked_per_class_names()))\n",
    "# Per class feature importance values\n",
    "print('ranked per class feature values: {}'.format(global_explanation.get_ranked_per_class_values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out a dictionary that holds the sorted feature importance names and values\n",
    "print('global importance rank: {}'.format(global_explanation.get_feature_importance_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain overall model predictions as a collection of local (instance-level) explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature shap values for all features and all data points in the training data\n",
    "print('local importance values: {}'.format(global_explanation.local_importance_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate local explanations\n",
    "Explain local data points (individual instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can pass a specific data point or a group of data points to the explain_local function\n",
    "\n",
    "# E.g., Explain the first data point in the test set\n",
    "instance_num = 0\n",
    "local_explanation = explainer.explain_local(x_test[instance_num,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction for the first member of the test set and explain why model made that prediction\n",
    "prediction_value = clf.predict(x_test)[instance_num]\n",
    "\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()[prediction_value]\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()[prediction_value]\n",
    "\n",
    "print('local importance values: {}'.format(sorted_local_importance_values))\n",
    "print('local importance names: {}'.format(sorted_local_importance_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Visualize'></a>\n",
    "## 5. Visualize\n",
    "Load the visualization dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import ExplanationDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ExplanationDashboard(global_explanation, model, dataset=x_test, true_y=y_test)\n",
    "except NameError as e:\n",
    "    # If we used sklearn model instead - show the dashboard with sk_model\n",
    "    ExplanationDashboard(global_explanation, sk_model, dataset=x_test, true_y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "Learn more\n",
    "- [RAPIDS](https://rapids.ai/)\n",
    "- [RAPIDS on Medium](https://medium.com/rapids-ai)\n",
    "- [cuML on GitHub](https://github.com/rapidsai/cuml.git)\n",
    "- [cuML API Reference](https://docs.rapids.ai/api/cuml/stable/api.html)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "mesameki"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
